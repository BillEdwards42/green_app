# ==============================================================================
# SCRIPT: fetch_weather_integrated.py
# PURPOSE:
#   - Fetches real-time weather observations for power generation-relevant stations
#   - Updates regional CSV files with weather data
#   - Synchronizes with generation data using timestamps
# ==============================================================================
import os
import requests
import json
from pathlib import Path
from dotenv import load_dotenv
from datetime import datetime
import csv
import time
import fcntl
import pandas as pd
import sys

# --- Configuration ---
load_dotenv()
CWA_API_KEY = os.getenv("CWA_API_KEY")
if not CWA_API_KEY:
    CWA_API_KEY = "CWA-498C56D4-B151-4539-992D-B2CB97042454"

# API endpoint for real-time observations
BASE_API_URL = "https://opendata.cwa.gov.tw/api/v1/rest/datastore/O-A0003-001"

# Define paths
BASE_DIR = Path(__file__).parent
STRU_DATA_DIR = BASE_DIR / "stru_data"
LOGS_DIR = BASE_DIR / "logs"
WEATHER_LOG_FILE = LOGS_DIR / "10min_weather_log.csv"

# Stations relevant to power generation, grouped by region
# åŒ—éƒ¨åœ°å€ (North) - æ¶µè“‹ä¸»è¦éƒ½æœƒå€ã€æ²¿æµ·åœ°å€ï¼ˆé¢¨åŠ›ç™¼é›»ï¼‰åŠä¸»è¦ç©ºæ¸¯
# ä¸­éƒ¨åœ°å€ (Central) - åŒ…å«æ²¿æµ·é¢¨å ´ã€å¹³åŸå…‰é›»å€ã€é—œéµæ°´åŠ›ç™¼é›»æŒ‡æ¨™
# å—éƒ¨åœ°å€ (South) - é›†ä¸­åœ¨å˜‰å—èˆ‡é«˜å±å¹³åŸï¼Œå°ç£å¤ªé™½èƒ½ç™¼é›»é‡é®
# æ±éƒ¨åœ°å€ (East) - æ¶µè“‹å®œè˜­å¹³åŸèˆ‡èŠ±æ±ç¸±è°·ä¸»è¦äººå£èˆ‡è¾²æ¥­å€
# é›¢å³¶åœ°å€ (Islands) - ç¨ç«‹é›»ç¶²ï¼Œæ‡‰ç¨ç«‹çœ‹å¾…
STATIONS_BY_REGION = {
    "North": [
        "åŸºéš†", "æ·¡æ°´", "æ–°åŒ—", "æ–°ç«¹", "è‡ºåŒ—", "æ–°å±‹",
        "æ¡ƒåœ’è¾²æ”¹", "æ–‡å±±èŒ¶æ”¹", "æ–°åŸ”å·¥ä½œç«™"
    ],
    "Central": [
        "è‡ºä¸­", "æ¢§æ£²", "å¾Œé¾", "å¤å‘", "å½°å¸«å¤§", "éº¥å¯®",
        "ç”°ä¸­", "æ—¥æœˆæ½­", "è‹—æ —è¾²æ”¹"
    ],
    "South": [
        "å˜‰ç¾©", "è‡ºå—", "é«˜é›„", "æ†æ˜¥", "æ°¸åº·",
        "è‡ºå—è¾²æ”¹", "æ——å—è¾²æ”¹", "é«˜é›„è¾²æ”¹", "å±æ±"
    ],
    "East": [
        "å®œè˜­", "èŠ±è“®", "æˆåŠŸ", "è‡ºæ±", "å¤§æ­¦"
    ],
    "Islands": [
        "æ¾æ¹–", "é‡‘é–€", "é¦¬ç¥–", "æ±å‰å³¶", "è˜­å¶¼"
    ]
}

# Fields to extract and average
TARGET_FIELDS = ["SunshineDuration", "AirTemperature", "WindSpeed", "Precipitation"]

# All fields to extract for comprehensive station data
ALL_STATION_FIELDS = ["AirTemperature", "WindSpeed", "SunshineDuration", "Precipitation", "UVIndex", "WindDirection"]

# --- Helper Functions ---
def ensure_directories():
    """Create necessary directories if they don't exist."""
    STRU_DATA_DIR.mkdir(parents=True, exist_ok=True)
    LOGS_DIR.mkdir(parents=True, exist_ok=True)

def safe_float_convert(value):
    """Safely converts a value to float, handling CWA's null identifiers."""
    try:
        if float(value) < -90:
            return None
        return float(value)
    except (ValueError, TypeError):
        return None

def fetch_weather_data():
    """Fetches the latest weather observation data from the CWA API."""
    params = {"Authorization": CWA_API_KEY}
    print(f"ğŸ“¡ [{datetime.now().strftime('%H:%M:%S')}] Fetching real-time weather data...")
    try:
        response = requests.get(BASE_API_URL, params=params, timeout=30)
        response.raise_for_status()
        print(f"âœ… SUCCESS: API response received. Status: {response.status_code}")
        return response.json()
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTP ERROR: {e}")
    except Exception as e:
        print(f"âŒ UNEXPECTED ERROR during fetch: {e}")
    return None

def read_csv_with_lock(filepath):
    """Read CSV file with file locking."""
    with open(filepath, 'r') as f:
        fcntl.flock(f.fileno(), fcntl.LOCK_SH)
        try:
            df = pd.read_csv(f)
            return df
        finally:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)

def write_csv_with_lock(df, filepath, mode='w'):
    """Write CSV file with file locking."""
    with open(filepath, mode) as f:
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
        try:
            if mode == 'w':
                df.to_csv(f, index=False)
            else:
                df.to_csv(f, index=False, header=False)
        finally:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)

def round_to_nearest_10min(dt):
    """Round datetime to nearest 10-minute interval."""
    # Round down to nearest 10 minutes
    minutes = (dt.minute // 10) * 10
    return dt.replace(minute=minutes, second=0, microsecond=0)

def save_all_stations_data(all_stations_data, obs_datetime):
    """Save comprehensive weather data for all stations in wide format."""
    all_stations_file = STRU_DATA_DIR / "all_stations_weather.csv"
    
    # Prepare data dictionary with DateTime as first column
    row_data = {"DateTime": obs_datetime}
    
    # Process each station
    for station in all_stations_data:
        station_name = station.get("StationName")
        if not station_name:
            continue
            
        elements = station.get("WeatherElement", {})
        
        # Extract all fields for this station
        for field in ALL_STATION_FIELDS:
            column_name = f"{station_name}_{field}"
            
            if field == "Precipitation":
                # Special handling for precipitation (nested in "Now")
                value = elements.get("Now", {}).get("Precipitation")
            else:
                value = elements.get(field)
            
            # Convert to float, handling null values
            float_value = safe_float_convert(value)
            row_data[column_name] = float_value if float_value is not None else ""
    
    # Create DataFrame with single row
    new_df = pd.DataFrame([row_data])
    
    # Check if file exists and append or create new
    if all_stations_file.exists():
        try:
            # Read existing data
            existing_df = read_csv_with_lock(all_stations_file)
            
            # Check if this timestamp already exists
            if obs_datetime in existing_df['DateTime'].values:
                print(f"âš ï¸ Data for {obs_datetime} already exists in all_stations_weather.csv. Skipping.")
                return
            
            # Append new row
            combined_df = pd.concat([existing_df, new_df], ignore_index=True)
            write_csv_with_lock(combined_df, all_stations_file)
            print(f"âœ… Appended new data to all_stations_weather.csv")
        except Exception as e:
            print(f"âŒ Error updating all_stations_weather.csv: {e}")
    else:
        # Create new file
        write_csv_with_lock(new_df, all_stations_file)
        print(f"âœ… Created new all_stations_weather.csv with {len(row_data)-1} station metrics")

def get_latest_timestamp_from_csvs():
    """Get the most recent timestamp from regional CSV files."""
    latest_timestamp = None
    
    for region_file in STRU_DATA_DIR.glob("*.csv"):
        if region_file.name == "electricity_demand.csv":
            continue
            
        try:
            df = pd.read_csv(region_file)
            if not df.empty and 'Timestamp' in df.columns:
                df['Timestamp'] = pd.to_datetime(df['Timestamp'])
                file_latest = df['Timestamp'].max()
                if latest_timestamp is None or file_latest > latest_timestamp:
                    latest_timestamp = file_latest
        except Exception as e:
            print(f"âš ï¸ Could not read {region_file.name}: {e}")
    
    return latest_timestamp

def process_and_update_data(api_data):
    """Process weather data and update regional CSV files."""
    try:
        all_stations_data = api_data['records']['Station']
        if not all_stations_data:
            print("âš ï¸ WARNING: API response contains no station data.")
            return
    except KeyError:
        print("âŒ ERROR: Could not find valid 'records' or 'Station' structure in API data.")
        return
    
    # Extract common observation datetime from first station
    obs_datetime = None
    if all_stations_data:
        first_station = all_stations_data[0]
        obs_time_str = first_station.get("ObsTime", {}).get("DateTime")
        if obs_time_str:
            try:
                # Parse the datetime string and format it consistently
                from datetime import datetime as dt
                obs_dt = dt.fromisoformat(obs_time_str.replace('+08:00', ''))
                obs_datetime = obs_dt.strftime('%Y-%m-%d %H:%M:%S')
            except:
                obs_datetime = obs_time_str
    
    # Save comprehensive data for all stations
    if obs_datetime:
        print("\nğŸ“Š Saving comprehensive station data...")
        save_all_stations_data(all_stations_data, obs_datetime)

    # Get the latest timestamp from existing CSV files
    target_timestamp = get_latest_timestamp_from_csvs()
    if target_timestamp is None:
        print("âš ï¸ No existing generation data found. Weather update skipped.")
        return
    
    print(f"ğŸ“… Updating weather data for timestamp: {target_timestamp}")

    # Process individual station data for logging
    log_rows = []
    processed_stations = {}

    for station in all_stations_data:
        station_name = station.get("StationName")
        if not any(station_name in sl for sl in STATIONS_BY_REGION.values()):
            continue

        obs_time_str = station.get("ObsTime", {}).get("DateTime")
        elements = station.get("WeatherElement", {})
        
        temp = safe_float_convert(elements.get("AirTemperature"))
        wind = safe_float_convert(elements.get("WindSpeed"))
        sunshine = safe_float_convert(elements.get("SunshineDuration"))
        
        # Extract precipitation from nested "Now" structure
        precip_value = elements.get("Now", {}).get("Precipitation")
        precip = safe_float_convert(precip_value)
        
        # Extract additional fields
        uv_index = safe_float_convert(elements.get("UVIndex"))
        wind_dir = safe_float_convert(elements.get("WindDirection"))
        
        has_null = any(v is None for v in [temp, wind, sunshine, precip])
        
        processed_stations[station_name] = {
            "AirTemperature": temp,
            "WindSpeed": wind,
            "SunshineDuration": sunshine,
            "Precipitation": precip,
            "UVIndex": uv_index,
            "WindDirection": wind_dir,
            "ObsTime": obs_time_str
        }

        log_rows.append({
            "Timestamp": obs_time_str,
            "StationName": station_name,
            "AirTemperature": temp if temp is not None else 'NULL',
            "WindSpeed": wind if wind is not None else 'NULL',
            "SunshineDuration": sunshine if sunshine is not None else 'NULL',
            "Precipitation": precip if precip is not None else 'NULL',
            "HasNullValue": has_null
        })

    if not log_rows:
        print("âš ï¸ No relevant stations found in the fetched data.")
        return

    # Log individual station data
    log_file_exists = WEATHER_LOG_FILE.exists()
    with open(WEATHER_LOG_FILE, 'a', newline='', encoding='utf-8') as csvfile:
        fieldnames = ["Timestamp", "StationName", "AirTemperature", "WindSpeed", "SunshineDuration", "Precipitation", "HasNullValue"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not log_file_exists:
            writer.writeheader()
        writer.writerows(log_rows)
    print(f"ğŸ“ {len(log_rows)} station records logged")

    # Update regional CSV files with weather data
    print("\n" + "="*60)
    print("STARTING REGIONAL PROCESSING WITH DEBUG OUTPUT")
    print("="*60)
    
    for region, station_list in STATIONS_BY_REGION.items():
        csv_path = STRU_DATA_DIR / f"{region}.csv"
        
        # Skip "Other" region as it doesn't have weather stations
        if region == "Other":
            continue
        
        # Calculate regional averages
        regional_averages = {}
        print(f"\nğŸ” DEBUG: Processing {region} region")
        print(f"   Stations in region: {station_list}")
        sys.stdout.flush()
        
        for field in TARGET_FIELDS:
            print(f"\n   ğŸ“Š Field: {field}")
            
            # Collect values with detailed logging
            station_values = []
            for s_name in station_list:
                if s_name in processed_stations:
                    value = processed_stations[s_name][field]
                    obs_time = processed_stations[s_name]['ObsTime']
                    station_values.append((s_name, value))
                    print(f"      - {s_name}: {value if value is not None else 'NULL'} (ObsTime: {obs_time})")
                else:
                    print(f"      - {s_name}: NOT FOUND in processed stations")
            
            # Filter out None values for average calculation
            valid_values = [v for name, v in station_values if v is not None]
            
            if valid_values:
                average = sum(valid_values) / len(valid_values)
                regional_averages[field] = round(average, 2)
                print(f"      â¡ï¸  Average ({len(valid_values)} valid stations): {regional_averages[field]}")
            else:
                regional_averages[field] = None
                print(f"      â¡ï¸  Average: None (no valid data)")

        # Update CSV file
        if csv_path.exists():
            try:
                df = read_csv_with_lock(csv_path)
                
                # Debug East region specifically
                if region == 'East':
                    print(f"   -> East.csv debug: {len(df)} rows, columns: {list(df.columns)}")
                    if not df.empty:
                        print(f"   -> East last row Total_Generation: {df.iloc[-1].get('Total_Generation', 'N/A')}")
                
                if df.empty:
                    print(f"âš ï¸ {region}.csv is empty. Skipping weather update.")
                    continue
                
                # Convert timestamps to datetime for comparison
                df['Timestamp'] = pd.to_datetime(df['Timestamp'])
                target_timestamp_dt = pd.to_datetime(target_timestamp)
                
                # Find the most recent row (last row) instead of exact timestamp match
                # This is more flexible and handles timezone/format differences
                last_idx = df.index[-1]  # Get last row
                
                # Update weather columns in the last row
                df.loc[last_idx, 'AirTemperature'] = regional_averages.get('AirTemperature')
                df.loc[last_idx, 'WindSpeed'] = regional_averages.get('WindSpeed') 
                df.loc[last_idx, 'SunshineDuration'] = regional_averages.get('SunshineDuration')
                df.loc[last_idx, 'Precipitation'] = regional_averages.get('Precipitation')
                
                # Debug output showing what values are being written
                print(f"\n   ğŸ’¾ Writing to CSV for {region}:")
                print(f"      - AirTemperature: {regional_averages.get('AirTemperature')}")
                print(f"      - WindSpeed: {regional_averages.get('WindSpeed')}")
                print(f"      - SunshineDuration: {regional_averages.get('SunshineDuration')}")
                print(f"      - Precipitation: {regional_averages.get('Precipitation')}")
                
                write_csv_with_lock(df, csv_path)
                print(f"âœ… Updated weather data for {region} (last entry: {df.loc[last_idx, 'Timestamp']})")
                    
            except Exception as e:
                print(f"âŒ ERROR updating {region}: {e}")
        else:
            print(f"âš ï¸ No generation data file found for {region}. Skipping weather update.")

def main():
    """Main execution function."""
    print("\n" + "="*60)
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Starting weather fetch...")
    
    ensure_directories()
    
    # Fetch and process weather data
    api_response_data = fetch_weather_data()
    if api_response_data:
        process_and_update_data(api_response_data)
    
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] Weather fetch completed!")
    print("="*60 + "\n")

if __name__ == "__main__":
    main()